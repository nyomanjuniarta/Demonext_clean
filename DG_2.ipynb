{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar\n",
    "\n",
    "# glawinette column number\n",
    "lemma1 = 0\n",
    "lemma2 = 1\n",
    "cat1 = 2\n",
    "cat2 = 3\n",
    "origine_morpho = 4\n",
    "origine_def = 5\n",
    "BAP1 = 6\n",
    "BAP2 = 7\n",
    "BAPsize = 8\n",
    "FAP1 = 9\n",
    "FAP2 = 10\n",
    "FAPsize = 11\n",
    "radical = 12\n",
    "FAPtype = 13\n",
    "\n",
    "# demonette column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_needed = ['DG-graph-binary', 'DG-contexts', 'DG-posets', 'DG-subposets']\n",
    "for f in folders_needed:\n",
    "    if not os.path.exists(f):\n",
    "        os.makedirs(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAPconverter(input_fap):\n",
    "    return input_fap.replace('(.+)', 'X').replace('$', '').replace('^', '')\n",
    "\n",
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        if cat[1] == 'p':  # nom propre\n",
    "            return 'Np'\n",
    "        return 'N'  # nom\n",
    "    return cat\n",
    "\n",
    "header = ''\n",
    "glawi_dict = dict()\n",
    "with codecs.open('glawinette-series.csv', 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if line_num >= 1:\n",
    "            elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "            glawi_dict[(elements[lemma1], elements[lemma2])] = FAPconverter(elements[FAP1]) + '-' + FAPconverter(elements[FAP2])\n",
    "            glawi_dict[(elements[lemma2], elements[lemma1])] = FAPconverter(elements[FAP2]) + '-' + FAPconverter(elements[FAP1])\n",
    "print(len(glawi_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4ef78",
   "metadata": {},
   "source": [
    "# 1. Creation of binary file for each graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edc3fc",
   "metadata": {},
   "source": [
    "In a binary file, each node also contains the lexeme's frequency, obtained from COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "frcowvec_categories = {'Nm': 'NOM', 'Nf': 'NOM', 'Nmp': 'NOM', 'Nfp': 'NOM', 'Nx': 'NOM', 'More': 'NOM',\n",
    "                       'Npm': 'NAM', 'Npf': 'NAM', 'Npx': 'NAM', 'Npmp': 'NAM', 'Npfp': 'NAM',\n",
    "                       'IJ': 'INT', 'Adj': 'ADJ', 'V': 'VER', 'Num': 'NUM', 'Pro': 'PRO', 'Adv': 'ADV'}\n",
    "\n",
    "def frcowvec_cat_conversion(lexeme):\n",
    "    old_cat = lexeme.split('_')[-1]\n",
    "    new_cat = frcowvec_categories.get(old_cat, old_cat)\n",
    "    return lexeme.split('_')[0] + '_' + str(new_cat)\n",
    "\n",
    "frequencies = pd.read_csv('frequencies-frcowvec.csv', header=0, index_col=0)\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'DG-families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "if '.gitignore' in input_files:\n",
    "    input_files.remove('.gitignore')\n",
    "output_dir = 'DG-graph-binary'\n",
    "\n",
    "for input_file in input_files:\n",
    "    fam_id = input_file.split()[0]\n",
    "    group_id = fam_id.split('-')[0]\n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                va = elements[graph_1] + '_' + elements[cat_1]\n",
    "                vb = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(va, vb) or H.has_edge(vb, va):\n",
    "                    continue\n",
    "                try:\n",
    "                    freq_a = frequencies.loc[frcowvec_cat_conversion(va)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_a = 0\n",
    "                try:\n",
    "                    freq_b = frequencies.loc[frcowvec_cat_conversion(vb)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_b = 0\n",
    "                H.add_node(va, label=category_shortening(elements[cat_1]), frequency=freq_a)\n",
    "                H.add_node(vb, label=category_shortening(elements[cat_2]), frequency=freq_b)\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    if (elements[graph_1], elements[graph_2]) in glawi_dict.keys():\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_1], elements[graph_2])))\n",
    "                    else:\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2])\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    if (elements[graph_2], elements[graph_1]) in glawi_dict.keys():\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_2], elements[graph_1])))\n",
    "                    else:\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    if (elements[graph_1], elements[graph_2]) in glawi_dict.keys():\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_1], elements[graph_2])))\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation]\\\n",
    "                                  + '$' + glawi_dict.get((elements[graph_2], elements[graph_1])))\n",
    "                    else:\n",
    "                        H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                        H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "    graph_file = open(join(output_dir, fam_id), 'wb')\n",
    "    pickle.dump(H, graph_file)\n",
    "    graph_file.close()\n",
    "    print(input_file.split()[0], end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6037a1d",
   "metadata": {},
   "source": [
    "# 2. Creation of formal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'DG-graph-binary'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "if '.gitignore' in input_files:\n",
    "    input_files.remove('.gitignore')\n",
    "input_files.sort()\n",
    "input_files.sort()\n",
    "ignored = []\n",
    "for i in ignored:\n",
    "    try:\n",
    "        input_files.remove(i)\n",
    "    except ValueError:\n",
    "        pass\n",
    "print(len(ignored), 'ignored')\n",
    "print(len(input_files), 'families')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78aa344",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count_dict = dict()\n",
    "for graph in input_files:\n",
    "    G2 = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    node_count_dict[graph] = len(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_compare(e1, e2):\n",
    "    if '$' in e1['label'] and '$' not in e2['label']:\n",
    "        return e1['label'].split('$')[0] == e2['label']\n",
    "    else:\n",
    "        return e1['label'] == e2['label']\n",
    "\n",
    "context_folder = 'DG-contexts'\n",
    "subgroup_prev = ''\n",
    "context = pd.DataFrame()\n",
    "counter = -1\n",
    "input_files_count = len(input_files)\n",
    "for subgraph in input_files:\n",
    "    subgroup_id = subgraph.split('-')[0]\n",
    "    if subgroup_id == subgroup_prev:\n",
    "        continue\n",
    "    counter += 1\n",
    "    if counter < 2000:\n",
    "        continue\n",
    "    if counter % 1000 == 0 and counter > 2000:\n",
    "        context.index = input_files\n",
    "        context.to_csv(join(context_folder, 'DG-context-' + str(counter/1000) + '.csv'))\n",
    "        context = pd.DataFrame()\n",
    "    G2 = pickle.load(open(join(input_dir, subgraph), 'rb'))\n",
    "    G2_node_count = node_count_dict.get(subgraph)\n",
    "    supergroup_prev = ''\n",
    "    is_subgraph = 0\n",
    "    membership = [0] * len(input_files)\n",
    "    for counter2 in range(input_files_count-1, -1, -1):\n",
    "        if membership[counter2] == 1:\n",
    "            continue\n",
    "        supergraph = input_files[counter2]\n",
    "        supergroup_id = supergraph.split('-')[0]\n",
    "        if supergroup_id == subgroup_id:\n",
    "            membership[counter2] = 1\n",
    "            continue\n",
    "        if supergroup_id == supergroup_prev:\n",
    "            membership[counter2] = membership[counter2+1]\n",
    "            continue\n",
    "        print(subgraph + ' ' + supergraph + '        ', end='\\r')\n",
    "        G1_node_count = node_count_dict.get(supergraph)\n",
    "        if G1_node_count >= G2_node_count:\n",
    "            G1 = pickle.load(open(join(input_dir, supergraph), 'rb'))\n",
    "            GM = isomorphism.DiGraphMatcher(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'], edge_match=edge_compare)\n",
    "            if GM.subgraph_is_isomorphic():\n",
    "                membership[counter2] = 1\n",
    "#                 try:\n",
    "#                     membership_of_G1 = context[supergroup_id.replace('F', 'G')]\n",
    "#                     membership = (membership_of_G1 | membership).astype(int)\n",
    "#                 except KeyError:\n",
    "#                     pass\n",
    "        supergroup_prev = supergroup_id\n",
    "    subgroup_prev = subgroup_id\n",
    "    membership = pd.Series(membership, name=subgroup_id.replace('F', 'G'))\n",
    "    context = pd.concat([context, membership], axis=1)\n",
    "context.index = input_files\n",
    "context.to_csv(join(context_folder, 'DG-context-last.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_files = [f for f in listdir(context_folder) if isfile(join(context_folder, f))]\n",
    "if '.gitignore' in context_files:\n",
    "    context_files.remove('.gitignore')\n",
    "input_files.sort()\n",
    "context_files.sort()\n",
    "context_join = pd.read_csv(join(context_folder, 'DG-context-1.0.csv'), header=0, index_col=0)\n",
    "for i in ignored:\n",
    "    if i in context_join.index:\n",
    "        context_join.drop(index=i, inplace=True)\n",
    "for c in range(1, len(context_files)):\n",
    "    new_ctx = pd.read_csv(join(context_folder, context_files[c]), header=0, index_col=0)\n",
    "    for i in ignored:\n",
    "        if i in new_ctx.index:\n",
    "            new_ctx.drop(index=i, inplace=True)\n",
    "    context_join = pd.concat([context_join, new_ctx], axis=1)\n",
    "print('context shape:', context_join.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad84a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_join.to_csv('DG-context.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb848aab",
   "metadata": {},
   "source": [
    "# 3. Calculation of AOC-poset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8745934",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_without_header = 'context_without_header.csv'\n",
    "context_with_header = pd.read_csv('DG-context.csv', header=0, index_col=0)\n",
    "context_with_header.to_csv(context_without_header, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6652ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d DG-posets/aoc-simplified.dot -f SIMPLIFIED -z')\n",
    "#os.system('dot -Tpdf posets/families_simplified.dot -o posets/families_simplified.pdf')\n",
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d DG-posets/aoc-full.dot -f FULL -z')\n",
    "#os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d posets/families_minimal.dot -f MINIMAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba50ff8",
   "metadata": {},
   "source": [
    "# 4. Calculation of subposets (parents and children of each concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_with_header = pd.read_csv('DG-context.csv', header=0, index_col=0)\n",
    "col_names = context_with_header.columns\n",
    "family_ids = context_with_header.index\n",
    "\n",
    "families = [f for f in listdir('DG-families') if isfile(join('DG-families', f))]\n",
    "if '.gitignore' in families:\n",
    "    families.remove('.gitignore')\n",
    "families_dict = dict()  # contains the representative word for a given family\n",
    "for f in families:\n",
    "    elements = f.replace('.txt', '').split()\n",
    "    families_dict[elements[0]] = elements[1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dot = 'aoc-full.dot'\n",
    "simplified_dot = 'aoc-simplified.dot'\n",
    "directory = 'DG-posets'\n",
    "out_directory = 'DG-subposets'\n",
    "L1 = nx.DiGraph()\n",
    "L2 = nx.DiGraph()\n",
    "with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '->' in line:  # a line showing edges between concepts\n",
    "            elements = line.split()\n",
    "            L1.add_edge(elements[0], elements[2])   \n",
    "            L2.add_edge(elements[2], elements[0])  \n",
    "        elif 'shape' in line:  # a line describing a concept \n",
    "            L1.add_node(line.split()[0])\n",
    "            L2.add_node(line.split()[0])\n",
    "            \n",
    "group_prev = ''\n",
    "counter = 0\n",
    "for file_name in families:\n",
    "    #if file_name != 'F01317 abducteur.txt':\n",
    "        #continue\n",
    "    family_id = file_name.split()[0]\n",
    "    group_id = family_id.split('-')[0]\n",
    "    if group_id == group_prev:\n",
    "        continue\n",
    "    group_prev = group_id\n",
    "    \n",
    "    # find vertex that contains the intended family and its parents+children\n",
    "    try:\n",
    "        object_id = family_ids.get_loc(family_id)\n",
    "    except KeyError:\n",
    "        #  ignored families (e.g. too much Npx)\n",
    "        continue\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if ('Object ' + str(object_id) + '\\\\n') in line:\n",
    "                vertex = line.split()[0]\n",
    "                break\n",
    "    parents = nx.descendants(L1, vertex)\n",
    "    children = nx.descendants(L2, vertex)\n",
    "    selected_vertices = parents.union(children)\n",
    "    selected_vertices.add(vertex)\n",
    "    \n",
    "    # find all vertex connected to the intended family, and write to dot\n",
    "    out_file_name = 'poset_' + family_id + '_simplified' + '.dot'\n",
    "    out_file = codecs.open(join(out_directory, out_file_name), 'w')\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'graph' in line or 'rankdir' in line or line == '}':\n",
    "                out_file.write(line)\n",
    "                continue\n",
    "            if '->' in line:\n",
    "                elements = line.split()\n",
    "                v1 = elements[0]\n",
    "                v2 = elements[2]\n",
    "                if v1 in selected_vertices and v2 in selected_vertices:\n",
    "                    out_file.write(line)\n",
    "                continue\n",
    "            vertex_id = line.split()[0]\n",
    "            concept_id = re.search('<(.*)>', line).group(1)\n",
    "            if vertex_id in selected_vertices:\n",
    "                to_be_written = line.split('|')[0] + '|'\n",
    "                attribute_string = line.split('|')[1]\n",
    "                if 'Attribute' not in attribute_string: # empty intent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    attributes = attribute_string.split('\\\\n')\n",
    "                    for attribute in attributes:\n",
    "                        if attribute == '':\n",
    "                            continue\n",
    "                        to_be_written += col_names[int(attribute.split()[1])] + '\\\\n'\n",
    "                to_be_written += '|'\n",
    "                object_string = line.split('|')[2]\n",
    "                if 'Object' not in object_string: # empty extent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    objects = object_string.split('\\\\n')\n",
    "                    for obj in objects:\n",
    "                        if obj == '' or '}' in obj:\n",
    "                            continue\n",
    "                        to_be_written += families_dict[family_ids[int(obj.split()[1])]] + '\\\\n'\n",
    "                to_be_written += '}\"];\\n'\n",
    "                to_be_written = re.sub('\\(I.*\\)\\|', concept_id + '|', to_be_written)\n",
    "                to_be_written = to_be_written.replace(',fillcolor=orange', '').replace(',fillcolor=lightblue', '')\n",
    "                if vertex_id == vertex:\n",
    "                    to_be_written = to_be_written.replace('style=filled', 'style=filled,fillcolor=orange')\n",
    "                out_file.write(to_be_written)\n",
    "    out_file.close()\n",
    "    counter += 1\n",
    "    print(group_id, end='\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
