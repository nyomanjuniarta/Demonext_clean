{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620de09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea26f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'raw_csv_files'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "if '.gitignore' in input_files:\n",
    "    input_files.remove('.gitignore')\n",
    "input_files.sort()\n",
    "\n",
    "input_file_code = dict()\n",
    "input_file_code['converts'] = 'C'\n",
    "input_file_code['demonette1'] = 'D'\n",
    "input_file_code['mordan'] = 'M'\n",
    "\n",
    "input_file_code['denomCONVX'] = 'Na'\n",
    "input_file_code['denomPREFX'] = 'Nb'\n",
    "input_file_code['denomXaire'] = 'Nc'\n",
    "input_file_code['denomXal'] = 'Nd'\n",
    "input_file_code['denomXique'] = 'Ne'\n",
    "input_file_code['denomXSUF1'] = 'Nf'\n",
    "input_file_code['denomXSUF2'] = 'Ng'\n",
    "input_file_code['denomXSUF3'] = 'Nh'\n",
    "input_file_code['denomXSUF4'] = 'Ni'\n",
    "input_file_code['denomXSUF5'] = 'Nj'\n",
    "input_file_code['denomXSUF6'] = 'Nk'\n",
    "\n",
    "input_file_code['dimocXaie'] = 'Ma'\n",
    "input_file_code['dimocXat'] = 'Mb'\n",
    "input_file_code['dimocXet'] = 'Mc'\n",
    "input_file_code['dimocXier'] = 'Md'\n",
    "input_file_code['dimocXasmeXaste'] = 'Me'\n",
    "input_file_code['dimocXite'] = 'Mf'\n",
    "input_file_code['dimocXien'] = 'Mg'\n",
    "input_file_code['dimocXisme'] = 'Mh'\n",
    "input_file_code['dimocXiste'] = 'Mi'\n",
    "\n",
    "input_file_code['derifantiX'] = 'Ra'\n",
    "input_file_code['derifde1X'] = 'Rb'\n",
    "input_file_code['derifenX'] = 'Rc'\n",
    "input_file_code['derifinX'] = 'Rd'\n",
    "input_file_code['derifQUANTX'] = 'Re'\n",
    "input_file_code['derifreX'] = 'Rf'\n",
    "input_file_code['deriftriX'] = 'Rg'\n",
    "input_file_code['derifXable'] = 'Rh'\n",
    "input_file_code['derifXiser'] = 'Ri'\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce1e1b",
   "metadata": {},
   "source": [
    "# 1. Identify family of lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89db7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representative(K):\n",
    "    # a representative of a family: the root (no direct relation going in), or the node with the largest outdegree\n",
    "    in_degree_dict = dict()  # only counting 'des2as' and 'as2des'\n",
    "    for v in list(K.nodes):\n",
    "        in_degree_dict[v] = 0\n",
    "    for e in list(K.edges.data()):\n",
    "        if not ('NA' in e[2]['label'] or 'indirect' in e[2]['label']):\n",
    "            in_degree_dict[e[1]] += 1\n",
    "    roots = list()\n",
    "    for k in in_degree_dict:\n",
    "        if in_degree_dict[k] == 0:\n",
    "            roots.append(k)\n",
    "    if len(roots) == 1:\n",
    "        return roots[0].split('_')[0]\n",
    "    elif len(roots) == 0:\n",
    "        max_out_degree = 0\n",
    "        selected_node = ''\n",
    "        for n in list(K.nodes):\n",
    "            if K.out_degree(n) > max_out_degree:\n",
    "                max_out_degree = K.out_degree(n)\n",
    "                selected_node = n\n",
    "        return selected_node.split('_')[0]\n",
    "    elif len(roots) > 1:\n",
    "        max_out_degree = 0\n",
    "        selected_root = roots[0]\n",
    "        for r in roots:\n",
    "            if K.out_degree(r) > max_out_degree:\n",
    "                max_out_degree = K.out_degree(r)\n",
    "                selected_root = r\n",
    "        return selected_root.split('_')[0]\n",
    "    \n",
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        if cat[1] == 'p':  # nom propre\n",
    "            return 'Np'\n",
    "        return 'N'  # nom\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca3a73",
   "metadata": {},
   "source": [
    "Create arc between two lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a3eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6539 pairs\n",
      "6374 unique pairs\n"
     ]
    }
   ],
   "source": [
    "header = ''\n",
    "G = nx.DiGraph()\n",
    "number_of_pairs = 0\n",
    "number_of_unique_pairs = 0\n",
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                header = line.replace('\\n','')\n",
    "            elif line_num >= 2:\n",
    "                number_of_pairs += 1\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if G.has_edge(v1, v2) or G.has_edge(v2, v1):\n",
    "                    continue\n",
    "                G.add_node(elements[graph_1] + '_' + elements[cat_1], label=category_shortening(elements[cat_1]))\n",
    "                G.add_node(elements[graph_2] + '_' + elements[cat_2], label=category_shortening(elements[cat_2]))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    G.add_edge(v1, v2, label=elements[cstr_1] + '-' + elements[cstr_2]) # without complexity\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    G.add_edge(v2, v1, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    #sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    G.add_edge(v1, v2, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                    G.add_edge(v2, v1, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "                number_of_unique_pairs += 1\n",
    "print(number_of_pairs, 'pairs')\n",
    "print(number_of_unique_pairs, 'unique pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c287f7",
   "metadata": {},
   "source": [
    "A connected component is regarded as a family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76cefe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 families\n"
     ]
    }
   ],
   "source": [
    "conn_comps = list(nx.weakly_connected_components(G))\n",
    "number_of_families = len(conn_comps)\n",
    "print(number_of_families, 'families')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e3ea5",
   "metadata": {},
   "source": [
    "Comparing every pair of families to detect isomorphy groups\n",
    "\n",
    "O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d837b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.00% complete\n"
     ]
    }
   ],
   "source": [
    "checked = list()\n",
    "H = nx.Graph() # graph of graphs\n",
    "for fam1 in range(0, number_of_families):\n",
    "    printProgressBar(fam1 + 1, number_of_families, prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)\n",
    "    if fam1 in checked:\n",
    "        continue\n",
    "    H.add_node(fam1)\n",
    "    for fam2 in range(fam1 + 1, number_of_families):\n",
    "        if fam2 in checked:\n",
    "            continue\n",
    "        G1 = nx.subgraph(G, conn_comps[fam1])\n",
    "        G2 = nx.subgraph(G, conn_comps[fam2])\n",
    "        if nx.is_isomorphic(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                            edge_match=lambda e1,e2: e1['label'] == e2['label']):\n",
    "            checked.append(fam2)\n",
    "            H.add_edge(fam1, fam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9442fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 isomorphy groups\n"
     ]
    }
   ],
   "source": [
    "H_conn_comps = [c for c in sorted(nx.connected_components(H), key=len, reverse=False)]\n",
    "print(len(H_conn_comps), 'isomorphy groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56299967",
   "metadata": {},
   "source": [
    "Generate csv file for each family to folder \"D-families\", and\n",
    "\n",
    "Generate \"D_summary_of_families.txt\" that records the list of lexemes for each family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e6261d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'D-families'\n",
    "lexeme_dict = dict()\n",
    "f_summary = codecs.open('D_summary_of_families.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('family_id\\tnumber_of_lexemes\\tlexemes\\n')\n",
    "for fam_fam_id, fam_fam in enumerate(H_conn_comps):\n",
    "    for fam_id, fam in enumerate(fam_fam):\n",
    "        representative = find_representative(nx.subgraph(G, conn_comps[fam]))\n",
    "        if len(fam_fam) == 1:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0')\n",
    "        else:\n",
    "            family_title = 'F' + str(fam_fam_id).rjust(5, '0') + '-' + str(fam_id).rjust(len(str(len(fam_fam)-1)), '0')\n",
    "        f_summary.write(family_title + '\\t'\\\n",
    "                       + str(len(conn_comps[fam])) + '\\t' + str(conn_comps[fam]) + '\\n')\n",
    "        for lexeme in conn_comps[fam]:\n",
    "            filename = family_title + ' ' + representative + '.txt'\n",
    "            lexeme_dict[lexeme] = filename\n",
    "            f_out = codecs.open(join(output_folder, filename), 'w+', encoding='utf-8')\n",
    "            f_out.write(header + '\\tfichier_origine' + '\\n\\n')\n",
    "            f_out.close()\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e030d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num < 2:\n",
    "                continue\n",
    "            elements = line.replace(' ','').split('\\t')\n",
    "            lexeme1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "            if elements[complexite] not in ['simple', 'complexe', 'motiv-form', 'motiv-sem', 'accidentel']:\n",
    "                print('warning ', input_file)\n",
    "            output_filename = lexeme_dict[lexeme1]\n",
    "            f_out = codecs.open(join(output_folder, output_filename), 'a+', encoding='utf-8')\n",
    "            f_out.write(line.strip('\\n') + '\\t' + input_file_code.get(input_file.split('-')[0]) + '\\n')\n",
    "            f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6c9e6",
   "metadata": {},
   "source": [
    "# 2. Generate graphs (.dot files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee2e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_shortening(cat):\n",
    "    if cat != 'Num' and cat[0] == 'N':\n",
    "        if cat[1] == 'p':  # nom propre\n",
    "            return 'Np'\n",
    "        return 'N'  # nom\n",
    "    return cat\n",
    "\n",
    "def edge_writer(H):\n",
    "    ret_str = ''\n",
    "    edges = H.edges(data=True)\n",
    "    for e in edges:\n",
    "        cat0 = category_shortening(e[0].split('_')[1])\n",
    "        cat1 = category_shortening(e[1].split('_')[1])\n",
    "            \n",
    "        if H.get_edge_data(*e)['style'] == 'dotted': \n",
    "            ret_str += cat0 + ' -- ' + e[2]['label'].split(': ')[1] + ' -- ' + cat1 + '; '\n",
    "        elif H.get_edge_data(*e)['style'] == 'dashed':\n",
    "            ret_str += cat0 + ' -? ' + e[2]['label'].split(': ')[1] + ' -? ' + cat1 + '; '\n",
    "        else:\n",
    "            ret_str += cat0 + ' -> ' + e[2]['label'].split(': ')[1] + ' -> ' + cat1 + '; '\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5281114",
   "metadata": {},
   "source": [
    "Generate .dot file for each family, and \"D_summary_of_groups.txt\" that records the list of edges for each isomorphy group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbbfec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.00% complete\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'D-families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "if '.gitignore' in input_files:\n",
    "    input_files.remove('.gitignore')\n",
    "\n",
    "f_summary = codecs.open('D_summary_of_family_groups.txt', 'w+', encoding='utf-8')\n",
    "f_summary.write('group id\\tnumber of lexemes\\tnumber of edges\\tnumber of families\\tedges\\tfamilies\\n')\n",
    "number_of_edges = []\n",
    "number_of_families = []\n",
    "words = ''\n",
    "group_prec = ''\n",
    "family_count = 0\n",
    "H = nx.DiGraph()\n",
    "counter = 0\n",
    "for input_file in input_files:\n",
    "    group_id = input_file.split(' ')[0].split('-')[0]\n",
    "    if group_id != group_prec and group_prec != '':\n",
    "        f_summary.write(group_prec + '\\t' + str(len(H)) + '\\t' + str(H.size()) + '\\t' + str(family_count) + '\\t')\n",
    "        f_summary.write(edge_writer(H)[:-2])\n",
    "        f_summary.write('\\t' + words[:-2] + '\\n')\n",
    "        family_count = 0\n",
    "        words = ''\n",
    "    family_count += 1\n",
    "    group_prec = input_file.split(' ')[0].split('-')[0]\n",
    "    \n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(v1, v2) or H.has_edge(v2, v1):\n",
    "                    continue\n",
    "                H.add_node(v1, label=elements[graph_1] + '\\n' + category_shortening(elements[cat_1]))\n",
    "                H.add_node(v2, label=elements[graph_2] + '\\n' + category_shortening(elements[cat_2]))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = elements[fichier_origine] + ': ' + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    H.add_edge(v1, v2, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = elements[fichier_origine] + ': ' + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    H.add_edge(v2, v1, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'indirect':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = elements[fichier_origine] + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dotted', label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = elements[fichier_origine] + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dashed', label=edge_type)\n",
    "                else:\n",
    "                    print(input_file, elements[orientation])\n",
    "        try:\n",
    "            if not nx.is_weakly_connected(H):\n",
    "                print(input_file)\n",
    "        except:\n",
    "            print('null graph', input_file)\n",
    "    words += str(list(H.nodes())) + '; '\n",
    "    write_dot(H, join('D-graph', input_file.replace('.txt','.dot')))\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(input_files), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)\n",
    "f_summary.write(group_prec + '\\t' + str(len(H)) + '\\t' + str(H.size()) + '\\t' + str(family_count) + '\\t')\n",
    "f_summary.write(edge_writer(H)[:-2])\n",
    "f_summary.write('\\t' + words[:-2] + '\\n')\n",
    "f_summary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc09b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
