{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf8a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from difflib import SequenceMatcher\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from networkx.algorithms import isomorphism\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from utils import printProgressBar, category_shortening\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1f5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_png(target_folder):\n",
    "    dot_files = [f for f in listdir(target_folder) if isfile(join(target_folder, f)) and '.dot' in f]\n",
    "    counter = 0\n",
    "    for dot_file in dot_files:\n",
    "        os.system('dot -Tpng \"' + join(target_folder, dot_file) + '\" -o \"' + join(target_folder, dot_file.replace('.dot', '.png')) + '\"')\n",
    "        counter += 1\n",
    "        printProgressBar(counter, len(dot_files), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb0801",
   "metadata": {},
   "source": [
    "# 1. Graphs with red edges for false derivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_folder = 'D-families'\n",
    "spurious_folder = 'D-web-spurious'\n",
    "model_folder = 'D-web-reference'\n",
    "df = pd.read_excel('D_false_deriv.xlsx')\n",
    "selected_rows = df[df['spurious_node_freq'] < 1]\n",
    "selected_rows = selected_rows[selected_rows['parent_node_count'] > 2]\n",
    "selected_rows.to_excel('D_false_deriv_filtered.xlsx', index=False)\n",
    "selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(output_folder, in_dot_file_name, out_dot_file_name, spurious=''):\n",
    "    G = pickle.load(open(join('D-graph-binary', in_dot_file_name.split()[0]), 'rb'))\n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(family_folder, in_dot_file_name.replace('.dot', '.txt')), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(v1, v2) or H.has_edge(v2, v1):\n",
    "                    continue\n",
    "                H.add_node(v1, label=elements[graph_1] + '\\n' + category_shortening(elements[cat_1]) + ', ' + str(G.nodes[v1]['frequency']))\n",
    "                H.add_node(v2, label=elements[graph_2] + '\\n' + category_shortening(elements[cat_2]) + ', ' + str(G.nodes[v2]['frequency']))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = elements[fichier_origine] + ': ' + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    H.add_edge(v1, v2, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = elements[fichier_origine] + ': ' + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    H.add_edge(v2, v1, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'indirect':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = elements[fichier_origine] + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dotted', label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = elements[fichier_origine] + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dashed', label=edge_type)\n",
    "                else:\n",
    "                    print(input_file, elements[orientation])\n",
    "    if spurious == '':\n",
    "        write_dot(H, join(output_folder, out_dot_file_name))\n",
    "    else:\n",
    "        spurious_nodes = spurious.split(', ')\n",
    "        for spurious_node in spurious_nodes:\n",
    "            for in_edge in H.in_edges(spurious_node):\n",
    "                H.edges[in_edge]['color'] = 'red'\n",
    "                H.edges[in_edge]['fontcolor'] = 'red'\n",
    "            for out_edge in H.out_edges(spurious_node):\n",
    "                H.edges[out_edge]['color'] = 'red'\n",
    "                H.edges[out_edge]['fontcolor'] = 'red'\n",
    "            H.nodes[spurious_node]['color'] = 'red'\n",
    "            H.nodes[spurious_node]['fontcolor'] = 'red'\n",
    "        write_dot(H, join(output_folder, out_dot_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7274f",
   "metadata": {},
   "source": [
    "For the code below, if there's an error like `...has no attribute 'split'`, try to open the `D_false_deriv.xlsx` and save it. Then reexecute the code from the beginning of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for index, row in selected_rows.iterrows():\n",
    "    create_graph(spurious_folder, row['child'], row['parent'].split()[0] + '_' + row['child'].split()[0] + '.dot', row['spurious_node'])\n",
    "    counter += 1\n",
    "    printProgressBar(counter, selected_rows.shape[0], prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)\n",
    "\n",
    "list_of_parents = selected_rows['parent'].unique()\n",
    "counter = 0\n",
    "for parent in list_of_parents:\n",
    "    create_graph(model_folder, parent, parent)\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(list_of_parents), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_png(spurious_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_png(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b45016",
   "metadata": {},
   "source": [
    "# 2. Graphs with green edges for missing derivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c21ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3941, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_folder = 'D-families'\n",
    "missing_folder = 'D-web-missing'\n",
    "model_folder = 'D-web-reference'\n",
    "df = pd.read_excel('D_missing_deriv.xlsx')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = pd.read_csv('frequencies-frcowvec.csv', header=0, index_col=0)\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b495ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "frcowvec_categories = {'Nm': 'NOM', 'Nf': 'NOM', 'Nmp': 'NOM', 'Nfp': 'NOM', 'Nx': 'NOM', 'More': 'NOM',\n",
    "                       'Npm': 'NAM', 'Npf': 'NAM', 'Npx': 'NAM', 'Npmp': 'NAM', 'Npfp': 'NAM',\n",
    "                       'IJ': 'INT', 'Adj': 'ADJ', 'V': 'VER', 'Num': 'NUM', 'Pro': 'PRO', 'Adv': 'ADV'}\n",
    "\n",
    "def frcowvec_cat_conversion(lexeme):\n",
    "    old_cat = lexeme.split('_')[-1]\n",
    "    new_cat = frcowvec_categories.get(old_cat, old_cat)\n",
    "    return lexeme.split('_')[0] + '_' + new_cat\n",
    "\n",
    "def get_frequency(lexeme):\n",
    "    if '??' in lexeme:\n",
    "        return 0\n",
    "    else:\n",
    "        try:\n",
    "            freq = frequencies.loc[frcowvec_cat_conversion(lexeme)]['freq']\n",
    "            return freq\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "def guess_missing_lexeme(in_a1, in_a2, in_b1):  # to be refined\n",
    "    # input_str ex : \"micocoulier_Nm : micocoule_Nf = cotonéaster_Nm : ?, micocoulier_Nm : micocouleraie_Nf = cotonéaster_Nm : ?\"\n",
    "    a1 = '{' + in_a1.split('_')[0] + '}'\n",
    "    a2 = '{' + in_a2.split('_')[0] + '}'\n",
    "    b1 = '{' + in_b1.split('_')[0] + '}'\n",
    "    a2_cat = in_a2.split('_')[1]\n",
    "    match = SequenceMatcher(None, a1, a2).find_longest_match(0, len(a1), 0, len(a2))\n",
    "    common = a1[match.a:match.a+match.size]\n",
    "    a1_suffix = a1.replace(common, '')\n",
    "    a2_suffix = a2.replace(common, '')\n",
    "    b2 = b1.replace(a1_suffix, a2_suffix)\n",
    "    if b2 == b1 and a1 != a2:\n",
    "        return '??'\n",
    "    else:\n",
    "        return b2.replace('{', '').replace('}', '') + '_' + a2_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_for_missing_lexemes(parent, child, output_folder, output_filename):\n",
    "    H = nx.DiGraph()\n",
    "    if parent == '':\n",
    "        to_be_generated = child\n",
    "    else:\n",
    "        to_be_generated = parent\n",
    "    G = pickle.load(open(join('D-graph-binary', to_be_generated.split()[0]), 'rb'))\n",
    "    with codecs.open(join(family_folder, to_be_generated.replace('dot', 'txt').replace(' **', '')), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                v1 = elements[graph_1] + '_' + elements[cat_1]\n",
    "                v2 = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(v1, v2) or H.has_edge(v2, v1):\n",
    "                    continue\n",
    "                H.add_node(v1, label=elements[graph_1] + '\\n' + category_shortening(elements[cat_1]) + ', ' + str(G.nodes[v1]['frequency']))\n",
    "                H.add_node(v2, label=elements[graph_2] + '\\n' + category_shortening(elements[cat_2]) + ', ' + str(G.nodes[v2]['frequency']))\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    edge_type = elements[fichier_origine] + ': ' + elements[cstr_1] + '-' + elements[cstr_2]\n",
    "                    H.add_edge(v1, v2, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    edge_type = elements[fichier_origine] + ': ' + elements[cstr_2] + '-' + elements[cstr_1]\n",
    "                    H.add_edge(v2, v1, label=edge_type, style='')\n",
    "                elif elements[orientation] == 'indirect':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = elements[fichier_origine] + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dotted', label=edge_type)\n",
    "                elif elements[orientation] == 'NA':\n",
    "                    sorted_lex = sorted([v1, v2])\n",
    "                    sorted_cstr = sorted([elements[cstr_1], elements[cstr_2]])\n",
    "                    edge_type = elements[fichier_origine] + ': ' + sorted_cstr[0] + '-' + sorted_cstr[1]\n",
    "                    H.add_edge(sorted_lex[0], sorted_lex[1], dir='none', style='dashed', label=edge_type)\n",
    "    if parent == '':\n",
    "        write_dot(H, join(output_folder, output_filename))\n",
    "        return\n",
    "    if '**' in parent:\n",
    "        G_parent = pickle.load(open(join('D-graph-binary', parent.split()[0]), 'rb'))\n",
    "        G_child = pickle.load(open(join('D-graph-binary', child.split()[0]), 'rb'))\n",
    "        GM = isomorphism.DiGraphMatcher(G_child, G_parent, node_match=lambda v1,v2: v1['label'] == v2['label'], edge_match=lambda e1,e2: e1['label'] == e2['label'])\n",
    "        node_diff = set()\n",
    "        for subgraph in GM.subgraph_isomorphisms_iter():\n",
    "            node_diff = G_child.nodes - subgraph\n",
    "        #print('subgraph', subgraph)\n",
    "        #print('node_diff', node_diff)\n",
    "        for n in node_diff:\n",
    "            #print('in:', list(G_child.in_edges(n, data=True)))\n",
    "            #print('out:', list(G_child.out_edges(n, data=True)))\n",
    "            new_lexeme = n\n",
    "            if len(G_child.in_edges(n)) > 0:\n",
    "                one_origin = list(G_child.in_edges(n))[0][0]\n",
    "                try:\n",
    "                    new_lexeme = guess_missing_lexeme(one_origin, n, subgraph.get(one_origin))\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "            else:\n",
    "                one_derived = list(G_child.out_edges(n))[0][1]\n",
    "                try:\n",
    "                    new_lexeme = guess_missing_lexeme(one_derived, n, subgraph.get(one_derived))\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "            new_node_freq = get_frequency(new_lexeme)\n",
    "            if new_lexeme == '??':\n",
    "                new_lexeme = new_lexeme + '_' + n\n",
    "                H.add_node(new_lexeme, label='?? \\n' + category_shortening(n.split('_')[1]) + ', ' + str(new_node_freq), fontcolor='green', color='green')\n",
    "            else:\n",
    "                H.add_node(new_lexeme, label=new_lexeme.split('_')[0] + '\\n' + category_shortening(n.split('_')[1]) + ', ' + str(new_node_freq), fontcolor='green', color='green')\n",
    "            for i in list(G_child.in_edges(n, data=True)):\n",
    "                if 'NA' in i[2].get('label') and subgraph.get(i[0]) is not None:\n",
    "                    H.add_edge(subgraph.get(i[0]), new_lexeme, label=i[2].get('label').split('_')[0], style='dashed', dir='none', fontcolor='green', color='green')\n",
    "                elif 'indirect' in i[2].get('label') and subgraph.get(i[0]) is not None:\n",
    "                    H.add_edge(subgraph.get(i[0]), new_lexeme, label=i[2].get('label').split('_')[0], style='dotted', dir='none', fontcolor='green', color='green')\n",
    "                elif subgraph.get(i[0]) is not None:\n",
    "                    H.add_edge(subgraph.get(i[0]), new_lexeme, label=i[2].get('label'), fontcolor='green', color='green')\n",
    "            for o in list(G_child.out_edges(n, data=True)):\n",
    "                if 'NA' in o[2].get('label') or 'indirect' in o[2].get('label'):\n",
    "                    continue\n",
    "                if subgraph.get(o[1]) is not None:\n",
    "                    H.add_edge(new_lexeme, subgraph.get(o[1]), label=o[2].get('label'), fontcolor='green', color='green')\n",
    "        write_dot(H, join(output_folder, output_filename))\n",
    "    else:\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd568604",
   "metadata": {},
   "source": [
    "For the code below, if there's an error like `...has no attribute 'split'`, try to open the `D_missing_deriv.xlsx` and save it. Then reexecute the code from the beginning of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ac104",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_child = df['child'].unique()\n",
    "counter = 0\n",
    "for child in list_of_child:\n",
    "    create_graph_for_missing_lexemes('', child, model_folder, child)\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(list_of_child), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32822539",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for index, row in df.iterrows():\n",
    "    create_graph_for_missing_lexemes(row['parent'], row['child'], missing_folder, row['parent'].split()[0] + '_' + row['child'].split()[0] + '.dot')\n",
    "    counter += 1\n",
    "    printProgressBar(counter, df.shape[0], prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_png(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_png(missing_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca2ad3",
   "metadata": {},
   "source": [
    "# Afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3869c4",
   "metadata": {},
   "source": [
    "In the end, we'll have the folders `D-web-missing`, `D-web-reference`, and `D-web-spurious` each populated with DOT and PNG files.\n",
    "\n",
    "Copy these PNG to `missing`, `reference`, and `spurious` folders, respectively, in the project `Demonext-web`.\n",
    "\n",
    "You should also copy the files `D_false_deriv_filtered.xlsx` and `D_missing_deriv.xlsx` to the root folder of `Demonext-web`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ba96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
