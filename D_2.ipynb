{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08708cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar, category_shortening\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab796a2",
   "metadata": {},
   "source": [
    "# 1. Creation of binary file for each graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9817d",
   "metadata": {},
   "source": [
    "In a binary file, each node also contains the lexeme's frequency, obtained from COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "frcowvec_categories = {'Nm': 'NOM', 'Nf': 'NOM', 'Nmp': 'NOM', 'Nfp': 'NOM', 'Nx': 'NOM', 'More': 'NOM',\n",
    "                       'Npm': 'NAM', 'Npf': 'NAM', 'Npx': 'NAM', 'Npmp': 'NAM', 'Npfp': 'NAM',\n",
    "                       'IJ': 'INT', 'Adj': 'ADJ', 'V': 'VER', 'Num': 'NUM', 'Pro': 'PRO', 'Adv': 'ADV'}\n",
    "\n",
    "def frcowvec_cat_conversion(lexeme):\n",
    "    old_cat = lexeme.split('_')[-1]\n",
    "    new_cat = frcowvec_categories.get(old_cat, old_cat)\n",
    "    return lexeme.split('_')[0] + '_' + str(new_cat)\n",
    "\n",
    "frequencies = pd.read_csv('frequencies-frcowvec.csv', header=0, index_col=0)\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'D-families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f)) and '.txt' in f]\n",
    "output_dir = 'D-graph-binary'\n",
    "\n",
    "for input_file in input_files:\n",
    "    fam_id = input_file.split()[0]\n",
    "    group_id = fam_id.split('-')[0]\n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                va = elements[graph_1] + '_' + elements[cat_1]\n",
    "                vb = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(va, vb) or H.has_edge(vb, va):\n",
    "                    continue\n",
    "                try:\n",
    "                    freq_a = frequencies.loc[frcowvec_cat_conversion(va)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_a = 0\n",
    "                try:\n",
    "                    freq_b = frequencies.loc[frcowvec_cat_conversion(vb)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_b = 0\n",
    "                H.add_node(va, label=category_shortening(elements[cat_1]), frequency=freq_a)\n",
    "                H.add_node(vb, label=category_shortening(elements[cat_2]), frequency=freq_b)\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2])\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                    H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "    graph_file = open(join(output_dir, fam_id), 'wb')\n",
    "    pickle.dump(H, graph_file)\n",
    "    graph_file.close()\n",
    "    print(input_file.split()[0], end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1963",
   "metadata": {},
   "source": [
    "# 2. Creation of formal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646cfe16",
   "metadata": {},
   "source": [
    "A family should be ignored (not included in the calculation of formal context, and consequently will not appear in the AOC-poset) if it's something like this:\n",
    "\n",
    "![saint-denis](Saint-Denis.png)\n",
    "\n",
    "Usually, this type of family contains numerous `Np` (proper noun) lexemes which are linked to one other lexeme, using the same construction (e.g. `X-Xien`).\n",
    "\n",
    "It may take hours (instead of seconds) just to check the subgraph of this type of family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any family that contains one of these lexemes is excluded\n",
    "ignored_list = ['\\'dionysien_Adj', 'salvadorien_Adj', '\\'sulpicien_Nm', '\\'mauricien_Adj', 'Obama_Npx',\n",
    "                'Gentilly_Npx', 'Sainte-Foy_Npx', 'fontenaysien_Adj', '\\'georgien_Adj', 'léodégarien_Adj',\n",
    "                'audonien_Adj', 'saint-andréen_Adj', 'Chicago_Npx', 'castelneuvien_Adj', '\\'moncéen_Adj'\n",
    "                'saint-victorien_Adj', 'saint-cyrien_Adj', '\\'oratorien_Adj', '\\'jovicien_Adj', '\\'chapelain_Nm',\n",
    "                'Pierre_Npm', 'saint-félicien_Nm', 'Taubira_Npx', '\\'laurentien_Adj', 'saint-gervaisien_Adj',\n",
    "                '\\'désidérien_Adj', '\\'macérien_Adj', '\\'Saint-Loup_Npx', '\\'lucéen_Adj', '\\'Moustier_Npx',\n",
    "                'LaBastide_Npx', '\\'condéen_Adj', 'Bussy_Npx', 'saint-hilairien_Adj', '\\'savignien_Adj',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'D-graph-binary'\n",
    "input_files = list()\n",
    "ignored_family_ids = list()\n",
    "print('Ignored families:')\n",
    "with codecs.open('D_summary_of_families.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if 'family_id' in line:\n",
    "            continue\n",
    "        family_id = line.split('\\t')[0]\n",
    "        ignored = False\n",
    "        for i in ignored_list:\n",
    "            if i in line:\n",
    "                print(line)\n",
    "                ignored_family_ids.append(family_id)\n",
    "                ignored = True\n",
    "                break\n",
    "        if not ignored:\n",
    "            input_files.append(family_id)\n",
    "print(len(ignored_family_ids), 'ignored')\n",
    "print(len(input_files), 'kept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count_dict = dict()\n",
    "for graph in input_files:\n",
    "    G2 = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    node_count_dict[graph] = len(G2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e8a0a",
   "metadata": {},
   "source": [
    "The cell below compare each pair of isomorphy groups, to decide whether a group is a subgraph of the other.\n",
    "\n",
    "This cell prints out the current pair being calculated.\n",
    "\n",
    "If it is stuck in a pair more than ~2 minutes, then one of this pair should be ignored, by putting one of its lexemes in the `ignored_list` variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c102403",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_folder = 'D-contexts'\n",
    "subgroup_prev = ''\n",
    "context = pd.DataFrame()\n",
    "counter = -1\n",
    "input_files_count = len(input_files)\n",
    "for subgraph in input_files:\n",
    "    subgroup_id = subgraph.split('-')[0]\n",
    "    if subgroup_id == subgroup_prev:\n",
    "        continue\n",
    "    counter += 1\n",
    "    if counter % 1000 == 0 and counter > 0:\n",
    "        context.index = input_files\n",
    "        context.to_csv(join(context_folder, 'D_context_' + str(counter/1000) + '.csv'))\n",
    "        context = pd.DataFrame()\n",
    "    G2 = pickle.load(open(join(input_dir, subgraph), 'rb'))\n",
    "    G2_node_count = node_count_dict.get(subgraph)\n",
    "    supergroup_prev = ''\n",
    "    is_subgraph = 0\n",
    "    membership = [0] * len(input_files)\n",
    "    for counter2 in range(input_files_count-1, -1, -1):\n",
    "        if membership[counter2] == 1:\n",
    "            continue\n",
    "        supergraph = input_files[counter2]\n",
    "        supergroup_id = supergraph.split('-')[0]\n",
    "        if supergroup_id == subgroup_id:\n",
    "            membership[counter2] = 1\n",
    "            continue\n",
    "        if supergroup_id == supergroup_prev:\n",
    "            membership[counter2] = membership[counter2+1]\n",
    "            continue\n",
    "        print(subgraph + ' ' + supergraph + '        ', end='\\r')\n",
    "        G1_node_count = node_count_dict.get(supergraph)\n",
    "        if G1_node_count >= G2_node_count:\n",
    "            G1 = pickle.load(open(join(input_dir, supergraph), 'rb'))\n",
    "            GM = isomorphism.DiGraphMatcher(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                                            edge_match=lambda e1,e2: e1['label'] == e2['label'])\n",
    "            if GM.subgraph_is_isomorphic():\n",
    "                membership[counter2] = 1\n",
    "        supergroup_prev = supergroup_id\n",
    "    subgroup_prev = subgroup_id\n",
    "    membership = pd.Series(membership, name=subgroup_id.replace('F', 'G'))\n",
    "    context = pd.concat([context, membership], axis=1)\n",
    "context.index = input_files\n",
    "context.to_csv(join(context_folder, 'D_context_last.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730292d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_files = [f for f in listdir(context_folder) if isfile(join(context_folder, f)) and 'context' in f]\n",
    "context_files.sort()\n",
    "context_join = pd.read_csv(join(context_folder, context_files[0]), header=0, index_col=0)\n",
    "for c in range(1, len(context_files)):\n",
    "    new_ctx = pd.read_csv(join(context_folder, context_files[c]), header=0, index_col=0)\n",
    "    context_join = pd.concat([context_join, new_ctx], axis=1, join='inner')\n",
    "print('context shape:', context_join.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d56d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_join.to_csv('D_context.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99645eeb",
   "metadata": {},
   "source": [
    "# 3. Calculation of AOC-poset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_without_header = 'D_context_without_header.csv'\n",
    "context_with_header = pd.read_csv('D_context.csv', header=0, index_col=0)\n",
    "context_with_header.to_csv(context_without_header, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d ' + join('D-posets', 'aoc_simplified.dot') + ' -f SIMPLIFIED -z')\n",
    "os.system('dot -Tpdf ' + join('D-posets', 'aoc_simplified.dot') + ' -o ' + join('D-posets', 'aoc_simplified.pdf'))\n",
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d ' + join('D-posets', 'aoc_full.dot') + ' -f FULL -z')\n",
    "os.system('dot -Tpdf ' + join('D-posets', 'aoc_full.dot') + ' -o ' + join('D-posets', 'aoc_full.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe867bf",
   "metadata": {},
   "source": [
    "# 4. Poset properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nx.DiGraph()\n",
    "with codecs.open(join('D-posets', 'aoc_simplified.dot'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if 'Attribute' in line:\n",
    "            L.add_node(line.split()[0])\n",
    "        elif '->' in line:\n",
    "            elements = line.split()\n",
    "            L.add_edge(elements[0], elements[2])\n",
    "count_in = 0\n",
    "sum_in = 0\n",
    "count_out = 0\n",
    "sum_out = 0\n",
    "no_edge = 0\n",
    "top = 0\n",
    "max_parent = 0\n",
    "max_child = 0\n",
    "min_parent = 1000\n",
    "min_child = 1000\n",
    "for node in L:\n",
    "    if L.in_degree(node) > max_child:\n",
    "        max_child = L.in_degree(node)\n",
    "    if L.out_degree(node) > max_parent:\n",
    "        max_parent = L.out_degree(node)\n",
    "    if L.in_degree(node) < min_child and L.in_degree(node) > 0:\n",
    "        min_child = L.in_degree(node)\n",
    "    if L.out_degree(node) < min_parent and L.out_degree(node) > 0:\n",
    "        min_parent = L.out_degree(node)\n",
    "    if L.in_degree(node) == 0 and L.out_degree(node) == 0:\n",
    "        no_edge += 1\n",
    "    if L.out_degree(node) == 0 and L.in_degree(node) > 0:\n",
    "        top += 1\n",
    "    if L.in_degree(node) > 0:\n",
    "        count_in += 1\n",
    "        sum_in += L.in_degree(node)\n",
    "    if L.out_degree(node) > 0:\n",
    "        count_out += 1\n",
    "        sum_out += L.out_degree(node)\n",
    "print('number of concepts:', len(L.nodes))\n",
    "print('number of isolated concepts (without parent/child):', no_edge)\n",
    "print('number of top concepts:', top)\n",
    "print('max_parent:', max_parent)\n",
    "print('max_child:', max_child)\n",
    "print('min_parent:', min_parent)\n",
    "print('min_child:', min_child)\n",
    "print('average number of parents:', (sum_out / count_out))\n",
    "print('average number of children:', (sum_in / count_in))\n",
    "print('number of levels:', len(nx.dag_longest_path(L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71391867",
   "metadata": {},
   "source": [
    "# 5. Calculation of subposets (parents and children of each concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_with_header = pd.read_csv('D_context.csv', header=0, index_col=0)\n",
    "col_names = context_with_header.columns\n",
    "family_ids = context_with_header.index\n",
    "\n",
    "families = [f for f in listdir('D-families') if isfile(join('D-families', f)) and f.startswith('F')]\n",
    "families_dict = dict()  # contains the representative word for a given family\n",
    "for f in families:\n",
    "    elements = f.replace('.txt', '').split()\n",
    "    families_dict[elements[0]] = elements[1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dot = 'aoc_full.dot'\n",
    "simplified_dot = 'aoc_simplified.dot'\n",
    "directory = 'D-posets'\n",
    "out_directory = 'D-subposets'\n",
    "L1 = nx.DiGraph()\n",
    "L2 = nx.DiGraph()\n",
    "with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '->' in line:  # a line showing edges between concepts\n",
    "            elements = line.split()\n",
    "            L1.add_edge(elements[0], elements[2])   \n",
    "            L2.add_edge(elements[2], elements[0])  \n",
    "        elif 'shape' in line:  # a line describing a concept \n",
    "            L1.add_node(line.split()[0])\n",
    "            L2.add_node(line.split()[0])\n",
    "            \n",
    "group_prev = ''\n",
    "counter = 0\n",
    "for file_name in families:\n",
    "    if not file_name.startswith('F'):\n",
    "        continue\n",
    "    #if file_name != 'F01317 abducteur.txt':\n",
    "        #continue\n",
    "    family_id = file_name.split()[0]\n",
    "    group_id = family_id.split('-')[0]\n",
    "    if group_id == group_prev:\n",
    "        continue\n",
    "    group_prev = group_id\n",
    "    \n",
    "    # find vertex that contains the intended family and its parents+children\n",
    "    try:\n",
    "        object_id = family_ids.get_loc(family_id)\n",
    "    except KeyError:\n",
    "        #  ignored families (e.g. too much Npx)\n",
    "        continue\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if ('Object ' + str(object_id) + '\\\\n') in line:\n",
    "                vertex = line.split()[0]\n",
    "                break\n",
    "    parents = nx.descendants(L1, vertex)\n",
    "    children = nx.descendants(L2, vertex)\n",
    "    selected_vertices = parents.union(children)\n",
    "    selected_vertices.add(vertex)\n",
    "    \n",
    "    # find all vertex connected to the intended family, and write to dot\n",
    "    out_file_name = 'poset_' + family_id + '_simplified' + '.dot'\n",
    "    out_file = codecs.open(join(out_directory, out_file_name), 'w')\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'graph' in line or 'rankdir' in line or line == '}':\n",
    "                out_file.write(line)\n",
    "                continue\n",
    "            if '->' in line:\n",
    "                elements = line.split()\n",
    "                v1 = elements[0]\n",
    "                v2 = elements[2]\n",
    "                if v1 in selected_vertices and v2 in selected_vertices:\n",
    "                    out_file.write(line)\n",
    "                continue\n",
    "            vertex_id = line.split()[0]\n",
    "            concept_id = re.search('<(.*)>', line).group(1)\n",
    "            if vertex_id in selected_vertices:\n",
    "                to_be_written = line.split('|')[0] + '|'\n",
    "                attribute_string = line.split('|')[1]\n",
    "                if 'Attribute' not in attribute_string: # empty intent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    attributes = attribute_string.split('\\\\n')\n",
    "                    for attribute in attributes:\n",
    "                        if attribute == '':\n",
    "                            continue\n",
    "                        to_be_written += col_names[int(attribute.split()[1])] + '\\\\n'\n",
    "                to_be_written += '|'\n",
    "                object_string = line.split('|')[2]\n",
    "                if 'Object' not in object_string: # empty extent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    objects = object_string.split('\\\\n')\n",
    "                    for obj in objects:\n",
    "                        if obj == '' or '}' in obj:\n",
    "                            continue\n",
    "                        to_be_written += families_dict[family_ids[int(obj.split()[1])]] + '\\\\n'\n",
    "                to_be_written += '}\"];\\n'\n",
    "                to_be_written = re.sub('\\(I.*\\)\\|', concept_id + '|', to_be_written)\n",
    "                to_be_written = to_be_written.replace(',fillcolor=orange', '').replace(',fillcolor=lightblue', '')\n",
    "                if vertex_id == vertex:\n",
    "                    to_be_written = to_be_written.replace('style=filled', 'style=filled,fillcolor=orange')\n",
    "                out_file.write(to_be_written)\n",
    "    out_file.close()\n",
    "    counter += 1\n",
    "    print(group_id, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dda30",
   "metadata": {},
   "source": [
    "Optional: convert each DOT to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c457df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_dir = 'D-subposets'\n",
    "dot_files = [f for f in listdir(dot_dir) if isfile(join(dot_dir, f)) and '.dot' in f]\n",
    "counter = 0\n",
    "for dot_file in dot_files:\n",
    "    os.system('dot -Tpdf \"' + join(dot_dir, dot_file) + '\" -o \"' + join(dot_dir, dot_file.replace('.dot', '.pdf')) + '\"')\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(dot_files), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eba781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
