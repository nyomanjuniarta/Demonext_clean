{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08708cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "from utils import printProgressBar, category_shortening\n",
    "\n",
    "# column number\n",
    "graph_1 = 3\n",
    "graph_2 = 6\n",
    "cat_1 = 8\n",
    "cat_2 = 10\n",
    "cstr_1 = 14\n",
    "cstr_2 = 17\n",
    "complexite = 19\n",
    "orientation = 21\n",
    "fichier_origine = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab796a2",
   "metadata": {},
   "source": [
    "# 1. Creation of binary file for each graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9817d",
   "metadata": {},
   "source": [
    "In a binary file, each node also contains the lexeme's frequency, obtained from COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "frcowvec_categories = {'Nm': 'NOM', 'Nf': 'NOM', 'Nmp': 'NOM', 'Nfp': 'NOM', 'Nx': 'NOM', 'More': 'NOM',\n",
    "                       'Npm': 'NAM', 'Npf': 'NAM', 'Npx': 'NAM', 'Npmp': 'NAM', 'Npfp': 'NAM',\n",
    "                       'IJ': 'INT', 'Adj': 'ADJ', 'V': 'VER', 'Num': 'NUM', 'Pro': 'PRO', 'Adv': 'ADV'}\n",
    "\n",
    "def frcowvec_cat_conversion(lexeme):\n",
    "    old_cat = lexeme.split('_')[-1]\n",
    "    new_cat = frcowvec_categories.get(old_cat, old_cat)\n",
    "    return lexeme.split('_')[0] + '_' + str(new_cat)\n",
    "\n",
    "frequencies = pd.read_csv('frequencies-frcowvec.csv', header=0, index_col=0)\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'D-families'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "if '.gitignore' in input_files:\n",
    "    input_files.remove('.gitignore')\n",
    "output_dir = 'D-graph-binary'\n",
    "\n",
    "for input_file in input_files:\n",
    "    fam_id = input_file.split()[0]\n",
    "    group_id = fam_id.split('-')[0]\n",
    "    H = nx.DiGraph()\n",
    "    with codecs.open(join(input_dir, input_file), 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line_num >= 2:\n",
    "                elements = line.replace('\\n','').replace(' ','').split('\\t')\n",
    "                va = elements[graph_1] + '_' + elements[cat_1]\n",
    "                vb = elements[graph_2] + '_' + elements[cat_2]\n",
    "                if H.has_edge(va, vb) or H.has_edge(vb, va):\n",
    "                    continue\n",
    "                try:\n",
    "                    freq_a = frequencies.loc[frcowvec_cat_conversion(va)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_a = 0\n",
    "                try:\n",
    "                    freq_b = frequencies.loc[frcowvec_cat_conversion(vb)]['freq']\n",
    "                except KeyError:\n",
    "                    freq_b = 0\n",
    "                H.add_node(va, label=category_shortening(elements[cat_1]), frequency=freq_a)\n",
    "                H.add_node(vb, label=category_shortening(elements[cat_2]), frequency=freq_b)\n",
    "                if elements[orientation] == 'as2de' or elements[orientation] == 'as2des':\n",
    "                    H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2])\n",
    "                elif elements[orientation] == 'de2as' or elements[orientation] == 'des2as':\n",
    "                    H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1])\n",
    "                else:\n",
    "                    H.add_edge(va, vb, label=elements[cstr_1] + '-' + elements[cstr_2] + '_' + elements[orientation])\n",
    "                    H.add_edge(vb, va, label=elements[cstr_2] + '-' + elements[cstr_1] + '_' + elements[orientation])\n",
    "    graph_file = open(join(output_dir, fam_id), 'wb')\n",
    "    pickle.dump(H, graph_file)\n",
    "    graph_file.close()\n",
    "    print(input_file.split()[0], end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1963",
   "metadata": {},
   "source": [
    "# 2. Creation of formal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b13826",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'D-graph-binary'\n",
    "input_files = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "if '.gitignore' in input_files:\n",
    "    input_files.remove('.gitignore')\n",
    "input_files.sort()\n",
    "ignored = []\n",
    "for i in ignored:\n",
    "    try:\n",
    "        input_files.remove(i)\n",
    "    except ValueError:\n",
    "        pass\n",
    "print(len(ignored), 'ignored')\n",
    "print(len(input_files), 'families')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count_dict = dict()\n",
    "for graph in input_files:\n",
    "    G2 = pickle.load(open(join(input_dir, graph), 'rb'))\n",
    "    node_count_dict[graph] = len(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c102403",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_folder = 'D-contexts'\n",
    "subgroup_prev = ''\n",
    "context = pd.DataFrame()\n",
    "counter = -1\n",
    "input_files_count = len(input_files)\n",
    "for subgraph in input_files:\n",
    "    subgroup_id = subgraph.split('-')[0]\n",
    "    if subgroup_id == subgroup_prev:\n",
    "        continue\n",
    "    counter += 1\n",
    "    if counter % 1000 == 0 and counter > 0:\n",
    "        context.index = input_files\n",
    "        context.to_csv(join(context_folder, 'D_context_' + str(counter/1000) + '.csv'))\n",
    "        context = pd.DataFrame()\n",
    "    G2 = pickle.load(open(join(input_dir, subgraph), 'rb'))\n",
    "    G2_node_count = node_count_dict.get(subgraph)\n",
    "    supergroup_prev = ''\n",
    "    is_subgraph = 0\n",
    "    membership = [0] * len(input_files)\n",
    "    for counter2 in range(input_files_count-1, -1, -1):\n",
    "        if membership[counter2] == 1:\n",
    "            continue\n",
    "        supergraph = input_files[counter2]\n",
    "        supergroup_id = supergraph.split('-')[0]\n",
    "        if supergroup_id == subgroup_id:\n",
    "            membership[counter2] = 1\n",
    "            continue\n",
    "        if supergroup_id == supergroup_prev:\n",
    "            membership[counter2] = membership[counter2+1]\n",
    "            continue\n",
    "        print(subgraph + ' ' + supergraph + '        ', end='\\r')\n",
    "        G1_node_count = node_count_dict.get(supergraph)\n",
    "        if G1_node_count >= G2_node_count:\n",
    "            G1 = pickle.load(open(join(input_dir, supergraph), 'rb'))\n",
    "            GM = isomorphism.DiGraphMatcher(G1, G2, node_match=lambda v1,v2: v1['label'] == v2['label'],\\\n",
    "                                            edge_match=lambda e1,e2: e1['label'] == e2['label'])\n",
    "            if GM.subgraph_is_isomorphic():\n",
    "                membership[counter2] = 1\n",
    "        supergroup_prev = supergroup_id\n",
    "    subgroup_prev = subgroup_id\n",
    "    membership = pd.Series(membership, name=subgroup_id.replace('F', 'G'))\n",
    "    context = pd.concat([context, membership], axis=1)\n",
    "context.index = input_files\n",
    "context.to_csv(join(context_folder, 'DG_context_last.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730292d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_files = [f for f in listdir(context_folder) if isfile(join(context_folder, f))]\n",
    "if '.gitignore' in context_files:\n",
    "    context_files.remove('.gitignore')\n",
    "input_files.sort()\n",
    "context_files.sort()\n",
    "context_join = pd.read_csv(join(context_folder, context_files[0]), header=0, index_col=0)\n",
    "for i in ignored:\n",
    "    if i in context_join.index:\n",
    "        context_join.drop(index=i, inplace=True)\n",
    "for c in range(1, len(context_files)):\n",
    "    new_ctx = pd.read_csv(join(context_folder, context_files[c]), header=0, index_col=0)\n",
    "    for i in ignored:\n",
    "        if i in new_ctx.index:\n",
    "            new_ctx.drop(index=i, inplace=True)\n",
    "    context_join = pd.concat([context_join, new_ctx], axis=1)\n",
    "print('context shape:', context_join.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d56d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_join.to_csv('D_context.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99645eeb",
   "metadata": {},
   "source": [
    "# 3. Calculation of AOC-poset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0211ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_without_header = 'D_context_without_header.csv'\n",
    "context_with_header = pd.read_csv('D_context.csv', header=0, index_col=0)\n",
    "context_with_header.to_csv(context_without_header, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979e5058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d ' + join('D-posets', 'aoc_simplified.dot') + ' -f SIMPLIFIED -z')\n",
    "os.system('dot -Tpdf ' + join('D-posets', 'aoc_simplified.dot') + ' -o ' + join('D-posets', 'aoc_simplified.pdf'))\n",
    "os.system('java -jar AOCPosetBuilder.jar -i ' + context_without_header + ' -a HERMES -d ' + join('D-posets', 'aoc_full.dot') + ' -f FULL -z')\n",
    "os.system('dot -Tpdf ' + join('D-posets', 'aoc_full.dot') + ' -o ' + join('D-posets', 'aoc_full.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71391867",
   "metadata": {},
   "source": [
    "# 4. Calculation of subposets (parents and children of each concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_with_header = pd.read_csv('D_context.csv', header=0, index_col=0)\n",
    "col_names = context_with_header.columns\n",
    "family_ids = context_with_header.index\n",
    "\n",
    "families = [f for f in listdir('D-families') if isfile(join('D-families', f))]\n",
    "if '.gitignore' in families:\n",
    "    families.remove('.gitignore')\n",
    "families_dict = dict()  # contains the representative word for a given family\n",
    "for f in families:\n",
    "    elements = f.replace('.txt', '').split()\n",
    "    families_dict[elements[0]] = elements[1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dot = 'aoc_full.dot'\n",
    "simplified_dot = 'aoc_simplified.dot'\n",
    "directory = 'D-posets'\n",
    "out_directory = 'D-subposets'\n",
    "L1 = nx.DiGraph()\n",
    "L2 = nx.DiGraph()\n",
    "with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if '->' in line:  # a line showing edges between concepts\n",
    "            elements = line.split()\n",
    "            L1.add_edge(elements[0], elements[2])   \n",
    "            L2.add_edge(elements[2], elements[0])  \n",
    "        elif 'shape' in line:  # a line describing a concept \n",
    "            L1.add_node(line.split()[0])\n",
    "            L2.add_node(line.split()[0])\n",
    "            \n",
    "group_prev = ''\n",
    "counter = 0\n",
    "for file_name in families:\n",
    "    #if file_name != 'F01317 abducteur.txt':\n",
    "        #continue\n",
    "    family_id = file_name.split()[0]\n",
    "    group_id = family_id.split('-')[0]\n",
    "    if group_id == group_prev:\n",
    "        continue\n",
    "    group_prev = group_id\n",
    "    \n",
    "    # find vertex that contains the intended family and its parents+children\n",
    "    try:\n",
    "        object_id = family_ids.get_loc(family_id)\n",
    "    except KeyError:\n",
    "        #  ignored families (e.g. too much Npx)\n",
    "        continue\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if ('Object ' + str(object_id) + '\\\\n') in line:\n",
    "                vertex = line.split()[0]\n",
    "                break\n",
    "    parents = nx.descendants(L1, vertex)\n",
    "    children = nx.descendants(L2, vertex)\n",
    "    selected_vertices = parents.union(children)\n",
    "    selected_vertices.add(vertex)\n",
    "    \n",
    "    # find all vertex connected to the intended family, and write to dot\n",
    "    out_file_name = 'poset_' + family_id + '_simplified' + '.dot'\n",
    "    out_file = codecs.open(join(out_directory, out_file_name), 'w')\n",
    "    with codecs.open(join(directory, simplified_dot), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if 'graph' in line or 'rankdir' in line or line == '}':\n",
    "                out_file.write(line)\n",
    "                continue\n",
    "            if '->' in line:\n",
    "                elements = line.split()\n",
    "                v1 = elements[0]\n",
    "                v2 = elements[2]\n",
    "                if v1 in selected_vertices and v2 in selected_vertices:\n",
    "                    out_file.write(line)\n",
    "                continue\n",
    "            vertex_id = line.split()[0]\n",
    "            concept_id = re.search('<(.*)>', line).group(1)\n",
    "            if vertex_id in selected_vertices:\n",
    "                to_be_written = line.split('|')[0] + '|'\n",
    "                attribute_string = line.split('|')[1]\n",
    "                if 'Attribute' not in attribute_string: # empty intent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    attributes = attribute_string.split('\\\\n')\n",
    "                    for attribute in attributes:\n",
    "                        if attribute == '':\n",
    "                            continue\n",
    "                        to_be_written += col_names[int(attribute.split()[1])] + '\\\\n'\n",
    "                to_be_written += '|'\n",
    "                object_string = line.split('|')[2]\n",
    "                if 'Object' not in object_string: # empty extent\n",
    "                    #to_be_written += '|'\n",
    "                    pass\n",
    "                else:\n",
    "                    objects = object_string.split('\\\\n')\n",
    "                    for obj in objects:\n",
    "                        if obj == '' or '}' in obj:\n",
    "                            continue\n",
    "                        to_be_written += families_dict[family_ids[int(obj.split()[1])]] + '\\\\n'\n",
    "                to_be_written += '}\"];\\n'\n",
    "                to_be_written = re.sub('\\(I.*\\)\\|', concept_id + '|', to_be_written)\n",
    "                to_be_written = to_be_written.replace(',fillcolor=orange', '').replace(',fillcolor=lightblue', '')\n",
    "                if vertex_id == vertex:\n",
    "                    to_be_written = to_be_written.replace('style=filled', 'style=filled,fillcolor=orange')\n",
    "                out_file.write(to_be_written)\n",
    "    out_file.close()\n",
    "    counter += 1\n",
    "    print(group_id, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dda30",
   "metadata": {},
   "source": [
    "Optional: convert each DOT to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c457df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_dir = 'D-subposets'\n",
    "dot_files = [f for f in listdir(dot_dir) if isfile(join(dot_dir, f))]\n",
    "if '.gitignore' in dot_files:\n",
    "    dot_files.remove('.gitignore')\n",
    "counter = 0\n",
    "for dot_file in dot_files:\n",
    "    os.system('dot -Tpdf \"' + join(dot_dir, dot_file) + '\" -o \"' + join(dot_dir, dot_file.replace('.dot', '.pdf')) + '\"')\n",
    "    counter += 1\n",
    "    printProgressBar(counter, len(dot_files), prefix = 'Progress:', suffix = 'complete', length = 50, decimals = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
